# Data Directory

This directory contains all the data files used and produced by the store clustering pipeline. The directory structure is organized to separate different types of data.

## Directory Structure

```
data/
├── internal/       # Internal sales and product data
├── external/       # External data sources
├── merging/        # Output from cluster merging process
└── raw/            # Raw data files before processing
```

## Environment Variables

The following environment variables can be set to override default paths:

- `DATA_DIR`: Base directory for all data files (default: `/workspaces/testing-dagster/data`)
- `INTERNAL_DATA_DIR`: Directory for internal data (default: `/workspaces/testing-dagster/data/internal`)
- `EXTERNAL_DATA_DIR`: Directory for external data (default: `/workspaces/testing-dagster/data/external`)
- `MERGING_DATA_DIR`: Directory for merged data (default: `/workspaces/testing-dagster/data/merging`)

## Key Files

### Internal Data

- `ns_map.csv`: Store category mapping file
- `ns_sales.csv`: Store sales data
- `sales_by_category.pkl`: Processed sales data by category
- `engineered_features.pkl`: Feature engineered dataset for internal data
- `clustering_models.pkl`: Trained clustering models for internal data
- `cluster_assignments.pkl`: Cluster assignments for internal data

### External Data

- `placerai.csv`: External data from PlaceAI
- `processed_external_data.pkl`: Processed external data
- `clustering_models.pkl`: Trained clustering models for external data
- `cluster_assignments.pkl`: Cluster assignments for external data

### Merging Data

- `merged_cluster_assignments.pkl`: Final merged cluster assignments

## Data Flow

1. Raw data is loaded from the `raw` directory
2. Preprocessing transforms and saves to the appropriate directories
3. Feature engineering is applied to the processed data
4. Models are trained and serialized
5. Cluster assignments are generated and saved
6. Internal and external clusters are merged

## Note

Some data files are automatically generated by the pipeline, while others need to be placed manually. Make sure to run the pipeline jobs in the correct order to ensure all dependencies are satisfied.
