{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a08d07-879a-41cf-84c4-8b2cccbe5700",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04732d32-4d18-442e-a67c-b5c929f6352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside config.py slfsvc-twa07\n",
      "printing snowflake connection dic {'tenant': 'fabb61b8-3afe-4e75-b934-a47f782b8cd7', 'client_id': '5617df5a-e4aa-4898-8260-9ac51b67e1c0', 'akssecret': 'prod-fspe-sp-secret', 'vault': 'kv-rtlfspekv0701slfsvc07', 'sfsecretkey': 'sfkey', 'sfkeypass': 'sfpasskey'}\n",
      "Initiating azcopy login\n",
      "az login succeeded\n"
     ]
    }
   ],
   "source": [
    "from fsutils import run_sf_sql as rp, config, email, adls_gen2, log\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# Get the SF connection and cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd688185-159a-41c6-8342-27f9b629db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside config.py slfsvc-twa07\n",
      "printing snowflake connection dic {'SF_USER_NAME': 'sa_slfsvc_fsca_app_twa_07', 'SF_ACCOUNT': 'cvsfrontstore.east-us-2.privatelink', 'SF_WAREHOUSE': 'WH_FSCA_TWA07_XL_BATCH', 'SF_USER_ROLE': 'ROLE_FSCA_TWA07_BATCH_FR', 'SF_DB': 'CORE_FS', 'SF_AUDIT_SCHEMA': 'WORK_FS'}\n",
      "printing sfparms {'SF_USER_NAME': 'sa_slfsvc_fsca_app_twa_07', 'SF_ACCOUNT': 'cvsfrontstore.east-us-2.privatelink', 'SF_WAREHOUSE': 'WH_FSCA_TWA07_XL_BATCH', 'SF_USER_ROLE': 'ROLE_FSCA_TWA07_BATCH_FR', 'SF_DB': 'CORE_FS', 'SF_AUDIT_SCHEMA': 'WORK_FS'}\n",
      "2025-04-15 14:01:31,443 - fsutils.run_sf_sql - INFO - Establishing Snowflake Connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsutils.run_sf_sql:Establishing Snowflake Connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting snowflake key file from Azure Key Vault\n",
      "Getting snowflake key pass from Azure Key Vault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:snowflake.connector.ocsp_snowflake:Failed to get OCSP response after 1 attempt. Consider checking for OCSP URLs being blocked\n",
      "WARNING:snowflake.connector.ocsp_snowflake:WARNING!!! Using fail-open to connect. Driver is connecting to an HTTPS endpoint without OCSP based Certificate Revocation checking as it could not obtain a valid OCSP Response to use from the CA OCSP responder. Details: \n",
      " {'driver': 'PythonConnector', 'version': '3.12.4', 'eventType': 'RevocationCheckFailure', 'eventSubType': 'OCSPResponseFailedToConnectCacheServer|OCSPResponseFetchFailure', 'sfcPeerHost': 'cvsfrontstore.east-us-2.privatelink.snowflakecomputing.com', 'certId': 'MEcwBwYFKw4DAhoEFKfEuLPcW7VYHqfX8TrFafVvSNeJBBR0hYDAZsffN97PvSk3qgMdvu3NFwIQASaIWIfhcMLpgMqT/CPqlQ==', 'ocspRequestBase64': 'ME8wTTBLMEkwRzAHBgUrDgMCGgQUp8S4s9xbtVgep9fxOsVp9W9I14kEFHSFgMBmx9833s+9KTeqAx2+7c0XAhABJohYh+FwwumAypP8I+qV', 'ocspResponderURL': 'http://ocsp.digicert.com', 'errorMessage': '254014: Failed to get OCSP response after 1 attempt.', 'insecureMode': False, 'failOpen': True, 'cacheEnabled': True, 'cacheHit': False}\n",
      "ERROR:snowflake.connector.ocsp_snowflake:Failed to get OCSP response after 1 attempt. Consider checking for OCSP URLs being blocked\n",
      "WARNING:snowflake.connector.ocsp_snowflake:WARNING!!! Using fail-open to connect. Driver is connecting to an HTTPS endpoint without OCSP based Certificate Revocation checking as it could not obtain a valid OCSP Response to use from the CA OCSP responder. Details: \n",
      " {'driver': 'PythonConnector', 'version': '3.12.4', 'eventType': 'RevocationCheckFailure', 'eventSubType': 'OCSPResponseFailedToConnectCacheServer|OCSPResponseFetchFailure|OCSPResponseFetchFailure', 'sfcPeerHost': 'cvsfrontstore.east-us-2.privatelink.snowflakecomputing.com', 'certId': 'MEcwBwYFKw4DAhoEFDnSi3H+HRm2X7PxKI8jvARZXEOVBBROIlQgGJXm427mD/r6uRLtBhePOQIQDPW9BitWAvR6uFAsI8zwZg==', 'ocspRequestBase64': 'ME8wTTBLMEkwRzAHBgUrDgMCGgQUOdKLcf4dGbZfs/EojyO8BFlcQ5UEFE4iVCAYlebjbuYP+vq5Eu0GF485AhAM9b0GK1YC9Hq4UCwjzPBm', 'ocspResponderURL': 'http://ocsp.digicert.com', 'errorMessage': '254014: Failed to get OCSP response after 1 attempt.', 'insecureMode': False, 'failOpen': True, 'cacheEnabled': True, 'cacheHit': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:01:54,871 - fsutils.run_sf_sql - INFO -  Getting Snowflake Cursor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsutils.run_sf_sql: Getting Snowflake Cursor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:01:54,872 - fsutils.run_sf_sql - INFO - Got the cursor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsutils.run_sf_sql:Got the cursor\n"
     ]
    }
   ],
   "source": [
    "conn, _=rp.get_connection(\"notebook-xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61514503-3808-4ba6-a5f8-ef51c10de7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus-az.cvshealth.com:9443/repository/pypi-proxy/simple/, https://nexus-az.cvshealth.com:9443/repository/Fsutils/simple, https://nexus-az.cvshealth.com:9443/repository/Niagara/simple\n",
      "Collecting openpyxl\n",
      "  Using cached https://nexus-az.cvshealth.com:9443/repository/pypi-proxy/packages/openpyxl/3.1.5/openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached https://nexus-az.cvshealth.com:9443/repository/pypi-proxy/packages/et-xmlfile/2.0.0/et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f00df6-8eec-4720-9299-778f5da2b698",
   "metadata": {},
   "source": [
    "# Write new items results to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1dde46-be75-40e7-8b8d-70efaf31e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the current date and time in ddmmyyy_hhmm format\n",
    "# now = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "# filename_suffix = f\"_{now}\"  # This will be appended to the filename\n",
    "\n",
    "# # Read Excel file\n",
    "# ns_data_new_items = pd.read_excel(\"final_need_states_new_items-20250407161801.xlsx\")\n",
    "\n",
    "# # Convert column names to uppercase\n",
    "# ns_data_new_items.columns = ns_data_new_items.columns.str.upper()\n",
    "\n",
    "# # Ensure all ATTRIBUTE columns are strings\n",
    "# for attribute in ['ATTRIBUTE_1', 'ATTRIBUTE_2', 'ATTRIBUTE_3', 'ATTRIBUTE_4', 'ATTRIBUTE_5', 'ATTRIBUTE_6']:\n",
    "#     ns_data_new_items[attribute] = ns_data_new_items[attribute].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b1820d-ef99-4eba-ace3-c0b6308804c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns_data_new_items.to_csv(\"temp.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82de545c-4a48-4f52-b98a-e413d8e8d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to SQL database with updated filename\n",
    "# write_pandas(conn, ns_data_new_items, \"NEED_STATES_NEW_ITEMS\" + filename_suffix, \n",
    "#              database=\"DL_FSCA_SLFSRV\", schema=\"TWA07\", auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed5689f-0f8e-491b-aa86-123f6ddd76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"NEED_STATES_NEW_ITEMS\" + filename_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ed6e7-ecd0-45ea-9c65-541d53a9a4b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Need States Input for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9dcb34-27e4-4b2d-9f9d-4b11ce4a5043",
   "metadata": {},
   "source": [
    "### *Filtering Matt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ffe4a8-bc53-4be6-bba4-431aa7536e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import string\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set up logging\n",
    "# ------------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Configuration: alternate names for product_id\n",
    "# ------------------------------------------------------------------------------\n",
    "ALTERNATE_PRODUCT_ID_COLUMNS = [\"item_nbr\", \"item_no_nbr\", \"item_id\"]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helper Function to Standardize \"to_be_dropped\"\n",
    "# ------------------------------------------------------------------------------\n",
    "def standardize_to_be_dropped(value):\n",
    "    \"\"\"\n",
    "    Remove punctuation, convert to lowercase, and trim whitespace.\n",
    "    \"\"\"\n",
    "    val_str = str(value)\n",
    "    # Remove punctuation\n",
    "    val_str_no_punc = val_str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Lowercase and strip\n",
    "    return val_str_no_punc.lower().strip()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main Processing Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def process_laundry_files(file_paths):\n",
    "    \"\"\"\n",
    "    Processes each Excel file in file_paths:\n",
    "      - Reads the file.\n",
    "      - Reconciles the product_id column (renames alternate columns if needed).\n",
    "      - Standardizes the 'to_be_dropped' column and logs any unexpected values.\n",
    "      - Filters rows where 'to_be_dropped' (standardized) equals 'no'.\n",
    "      - Adds a 'data_source' column with the file name.\n",
    "\n",
    "    Returns two DataFrames:\n",
    "      1. full_df: The union of all columns from the files (with data_source).\n",
    "      2. minimal_df: Only 'product_id' and 'data_source'.\n",
    "    \"\"\"\n",
    "    full_frames = []\n",
    "    minimal_frames = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            logger.info(f\"Successfully read file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- Reconcile product_id ---\n",
    "        if \"product_id\" not in df.columns:\n",
    "            found_alternate = False\n",
    "            for alt in ALTERNATE_PRODUCT_ID_COLUMNS:\n",
    "                if alt in df.columns:\n",
    "                    logger.warning(\n",
    "                        f\"File {file_path}: No 'product_id' column found. Renaming '{alt}' to 'product_id'.\"\n",
    "                    )\n",
    "                    df.rename(columns={alt: \"product_id\"}, inplace=True)\n",
    "                    found_alternate = True\n",
    "                    break\n",
    "            if not found_alternate:\n",
    "                logger.warning(f\"File {file_path}: No 'product_id' column found. Skipping file.\")\n",
    "                continue\n",
    "\n",
    "        # --- Standardize and check 'to_be_dropped' ---\n",
    "        if \"to_be_dropped\" not in df.columns:\n",
    "            logger.warning(f\"File {file_path}: 'to_be_dropped' column not found. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        df[\"to_be_dropped_std\"] = df[\"to_be_dropped\"].apply(standardize_to_be_dropped)\n",
    "        # Log unexpected values (only 'yes' or 'no' expected)\n",
    "        unexpected_vals = df.loc[\n",
    "            ~df[\"to_be_dropped_std\"].isin([\"yes\", \"no\"]), \"to_be_dropped_std\"\n",
    "        ].unique()\n",
    "        if len(unexpected_vals) > 0:\n",
    "            logger.warning(\n",
    "                f\"File {file_path}: Unexpected values in 'to_be_dropped': {list(unexpected_vals)}\"\n",
    "            )\n",
    "\n",
    "        # --- Filter rows where to_be_dropped is 'no'\n",
    "        df_filtered = df[df[\"to_be_dropped_std\"] == \"no\"].copy()\n",
    "        if df_filtered.empty:\n",
    "            logger.info(f\"File {file_path}: No rows with 'to_be_dropped' == 'no' after filtering.\")\n",
    "            continue\n",
    "\n",
    "        # --- Add data_source column ---\n",
    "        df_filtered[\"data_source\"] = file_path\n",
    "\n",
    "        # --- For full union output, drop the internal standardization column ---\n",
    "        if \"to_be_dropped_std\" in df_filtered.columns:\n",
    "            df_filtered.drop(columns=[\"to_be_dropped_std\"], inplace=True)\n",
    "\n",
    "        full_frames.append(df_filtered)\n",
    "\n",
    "        # --- Create minimal output (only product_id and data_source) ---\n",
    "        minimal_df = df_filtered[[\"product_id\", \"data_source\"]].copy()\n",
    "        minimal_frames.append(minimal_df)\n",
    "\n",
    "    # --- Concatenate all processed DataFrames ---\n",
    "    if full_frames:\n",
    "        full_df = pd.concat(full_frames, ignore_index=True, sort=False)\n",
    "    else:\n",
    "        full_df = pd.DataFrame()\n",
    "\n",
    "    if minimal_frames:\n",
    "        minimal_df = pd.concat(minimal_frames, ignore_index=True)\n",
    "    else:\n",
    "        minimal_df = pd.DataFrame(columns=[\"product_id\", \"data_source\"])\n",
    "\n",
    "    return full_df, minimal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f5141c8-4bbd-432e-83da-e738c5a53500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal output shape: (203, 2)\n",
      "Full output shape: (203, 15)\n"
     ]
    }
   ],
   "source": [
    "# Define the three Excel file paths\n",
    "file_paths = [\n",
    "        \"Fabric Enhancers.xlsx\",\n",
    "        \"Laundry Bleach.xlsx\",\n",
    "        \"Laundry.xlsx\",\n",
    "]\n",
    "\n",
    "# Process the files\n",
    "full_output_df, minimal_output_df = process_laundry_files(file_paths)\n",
    "\n",
    "# Write out the minimal file (only product_id and data_source)\n",
    "minimal_output_file = \"combined_product_ids.csv\"\n",
    "try:\n",
    "  minimal_output_df.to_csv(minimal_output_file, index=False)\n",
    "  logger.info(f\"Minimal output written to: {minimal_output_file}\")\n",
    "except Exception as e:\n",
    "  logger.error(f\"Error writing {minimal_output_file}: {e}\")\n",
    "\n",
    "# Write out the full union file (all unique columns plus data_source)\n",
    "full_output_file = \"combined_full.csv\"\n",
    "try:\n",
    "  full_output_df.to_csv(full_output_file, index=False)\n",
    "  logger.info(f\"Full union output written to: {full_output_file}\")\n",
    "except Exception as e:\n",
    "  logger.error(f\"Error writing {full_output_file}: {e}\")\n",
    "\n",
    "# Optionally, print the shapes of the outputs for verification\n",
    "print(\"Minimal output shape:\", minimal_output_df.shape)\n",
    "print(\"Full output shape:\", full_output_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3c9060-4f9d-4a72-a712-72bbaac23386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309865</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>493249</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436367</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634885</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>369415</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id            data_source\n",
       "0      309865  Fabric Enhancers.xlsx\n",
       "1      493249  Fabric Enhancers.xlsx\n",
       "2      436367  Fabric Enhancers.xlsx\n",
       "3      634885  Fabric Enhancers.xlsx\n",
       "4      369415  Fabric Enhancers.xlsx"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimal_output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132a9208-854a-4563-8792-6e8af02bf987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_description</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>Brand_standardized</th>\n",
       "      <th>Form_standardized</th>\n",
       "      <th>Load Count_standardized</th>\n",
       "      <th>Machine Compatibility_standardized</th>\n",
       "      <th>Price_standardized</th>\n",
       "      <th>Scent_standardized</th>\n",
       "      <th>New Item</th>\n",
       "      <th>to_be_dropped</th>\n",
       "      <th>out_of_scope</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309865</td>\n",
       "      <td>Downy Rinse &amp; Refresh Laundry Odor Remover and...</td>\n",
       "      <td>Product Description: Tired of stubborn odors t...</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Supplies</td>\n",
       "      <td>Downy</td>\n",
       "      <td>Liquid Fabric Softener</td>\n",
       "      <td>31-60</td>\n",
       "      <td>HE</td>\n",
       "      <td>10.01-15</td>\n",
       "      <td>Floral &amp; Botanical</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>493249</td>\n",
       "      <td>Bounce Free &amp; Gentle Fabric Softener Dryer She...</td>\n",
       "      <td>Product Description: Sensitive skin? Bounce Fr...</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Supplies</td>\n",
       "      <td>Other</td>\n",
       "      <td>Dryer Sheets</td>\n",
       "      <td>91-130</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>5.01-10</td>\n",
       "      <td>Unscented</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436367</td>\n",
       "      <td>Gain In-Wash Scent Booster Beads, Happy Hibisc...</td>\n",
       "      <td>Product Description: Sniff the good vibes. Tra...</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Supplies</td>\n",
       "      <td>Gain</td>\n",
       "      <td>Scent Enhancers</td>\n",
       "      <td>0-30</td>\n",
       "      <td>HE</td>\n",
       "      <td>5.01-10</td>\n",
       "      <td>Floral &amp; Botanical</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634885</td>\n",
       "      <td>Downy Light Scent In-Wash Scent Booster Beads,...</td>\n",
       "      <td>Product Description: Finally, long-lasting sce...</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Supplies</td>\n",
       "      <td>Downy</td>\n",
       "      <td>Scent Enhancers</td>\n",
       "      <td>0-30</td>\n",
       "      <td>HE</td>\n",
       "      <td>5.01-10</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>369415</td>\n",
       "      <td>Downy Fabric Softener Dryer Sheets, April Fres...</td>\n",
       "      <td>Product Description: Downy April Fresh Dryer S...</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Supplies</td>\n",
       "      <td>Downy</td>\n",
       "      <td>Dryer Sheets</td>\n",
       "      <td>91-130</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>5.01-10</td>\n",
       "      <td>Floral &amp; Botanical</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fabric Enhancers.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  \\\n",
       "0      309865  Downy Rinse & Refresh Laundry Odor Remover and...   \n",
       "1      493249  Bounce Free & Gentle Fabric Softener Dryer She...   \n",
       "2      436367  Gain In-Wash Scent Booster Beads, Happy Hibisc...   \n",
       "3      634885  Downy Light Scent In-Wash Scent Booster Beads,...   \n",
       "4      369415  Downy Fabric Softener Dryer Sheets, April Fres...   \n",
       "\n",
       "                                 product_description   category  \\\n",
       "0  Product Description: Tired of stubborn odors t...  Household   \n",
       "1  Product Description: Sensitive skin? Bounce Fr...  Household   \n",
       "2  Product Description: Sniff the good vibes. Tra...  Household   \n",
       "3  Product Description: Finally, long-lasting sce...  Household   \n",
       "4  Product Description: Downy April Fresh Dryer S...  Household   \n",
       "\n",
       "        subcategory Brand_standardized       Form_standardized  \\\n",
       "0  Laundry Supplies              Downy  Liquid Fabric Softener   \n",
       "1  Laundry Supplies              Other            Dryer Sheets   \n",
       "2  Laundry Supplies               Gain         Scent Enhancers   \n",
       "3  Laundry Supplies              Downy         Scent Enhancers   \n",
       "4  Laundry Supplies              Downy            Dryer Sheets   \n",
       "\n",
       "  Load Count_standardized Machine Compatibility_standardized  \\\n",
       "0                   31-60                                 HE   \n",
       "1                  91-130                        Unspecified   \n",
       "2                    0-30                                 HE   \n",
       "3                    0-30                                 HE   \n",
       "4                  91-130                        Unspecified   \n",
       "\n",
       "  Price_standardized  Scent_standardized New Item to_be_dropped out_of_scope  \\\n",
       "0           10.01-15  Floral & Botanical    False            No           No   \n",
       "1            5.01-10           Unscented    False            No           No   \n",
       "2            5.01-10  Floral & Botanical    False            No           No   \n",
       "3            5.01-10               Fresh    False            No           No   \n",
       "4            5.01-10  Floral & Botanical    False            No           No   \n",
       "\n",
       "             data_source  \n",
       "0  Fabric Enhancers.xlsx  \n",
       "1  Fabric Enhancers.xlsx  \n",
       "2  Fabric Enhancers.xlsx  \n",
       "3  Fabric Enhancers.xlsx  \n",
       "4  Fabric Enhancers.xlsx  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8574f471-f3ca-4cb0-8b87-dfdfea3bb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns_data = pd.read_excel(\"final_need_states_new_items-20250403122120.xlsx\")\n",
    "from fsutils import run_sf_sql as rp, config, email, adls_gen2, log\n",
    "conn, _=rp.get_connection(\"notebook-xlarge\")\n",
    "ns_data = pd.read_sql(\"select * from DL_FSCA_SLFSRV.TWA07.FINAL_NEED_STATES_NEW_ITEMS\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f98e6c0-4f0c-44e2-bac9-6cf5291d818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns_data = pd.read_excel(\"final_need_states_new_items-20250407161801.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ea6cfd7-3812-4d47-a7be-d6f4a3b8d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns_data.loc[ns_data['CATEGORY'].isin(['Hair Care - Hair Styling', 'Hair Care - Shampoo and Conditioner']), 'CATEGORY'] = 'Hair Care'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f5ecbf-4a5a-4009-bea9-7ad9dfb33211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_data = ns_data.query(\"`New Item` == False\")\n",
    "\n",
    "ns_data.columns = ns_data.columns.str.upper() #make column names uppercase\n",
    "\n",
    "ns_data = ns_data[['PRODUCT_ID', 'CATEGORY', 'NEED_STATE', 'CDT', 'ATTRIBUTE_1',\n",
    "       'ATTRIBUTE_2', 'ATTRIBUTE_3', 'ATTRIBUTE_4', 'ATTRIBUTE_5',\n",
    "       'ATTRIBUTE_6',]].drop_duplicates()\n",
    "\n",
    "ns_data = ns_data[~ns_data['NEED_STATE'].isna()]\n",
    "\n",
    "# Remember to change the date!\n",
    "ns_data.to_csv(\"/home/jovyan/fsassortment/store_clustering/data/need_states_mapping_20250414_AM.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b8cfb-071d-4924-a454-0ef272b4a75d",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf65fc30-5328-40b1-b658-96fc8e7f6ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57/53748760.py:4: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.numeric.Int64Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  write_pandas(conn, ns_data, \"NEED_STATES_20250414_AM\", database=\"DL_FSCA_SLFSRV\", schema=\"TWA07\", auto_create_table=True, overwrite=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 1,\n",
       " 14677,\n",
       " [('ximinalaak/file0.txt',\n",
       "   'LOADED',\n",
       "   14677,\n",
       "   14677,\n",
       "   1,\n",
       "   0,\n",
       "   None,\n",
       "   None,\n",
       "   None,\n",
       "   None)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for attribute in ['ATTRIBUTE_1', 'ATTRIBUTE_2', 'ATTRIBUTE_3', 'ATTRIBUTE_4', 'ATTRIBUTE_5', 'ATTRIBUTE_6']:\n",
    "    ns_data[attribute] = ns_data[attribute].astype(str)\n",
    "\n",
    "write_pandas(conn, ns_data, \"NEED_STATES_20250414_AM\", database=\"DL_FSCA_SLFSRV\", schema=\"TWA07\", auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a44792-34f5-416a-88b9-ff1b7ad26fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
